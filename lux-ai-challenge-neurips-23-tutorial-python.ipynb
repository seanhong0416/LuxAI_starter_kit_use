{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lux AI Season 2 @NeurIPS'23 Tutorial - Python Kit\n\nWelcome to Lux AI Season 2!\n\nThis notebook is the basic setup to use Jupyter Notebooks and the kaggle-environments package to develop your bot. If you plan to not use Jupyter Notebooks or any other programming language, please see our Github. The following are some important links!\n\nCompetition Page: https://www.kaggle.com/competitions/lux-ai-season-2-neurips-stage-2/\n\nOnline Visualizer: https://s2vis.lux-ai.org/\n\nSpecifications: https://www.lux-ai.org/specs-s2\n\nGithub: https://github.com/Lux-AI-Challenge/Lux-Design-S2\n\nBot API: https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits\n\nAnd if you haven't done so already, we highly recommend you join our Discord server at https://discord.gg/aWJt3UAcgn or at the minimum follow the kaggle forums at https://www.kaggle.com/c/lux-ai-season-2/discussion. We post important announcements there such as changes to rules, events, and opportunities from our sponsors!\n\nNow let's get started!","metadata":{}},{"cell_type":"markdown","source":"## Prerequisites\nWe assume that you have a basic knowledge of Python and programming. It's okay if you don't know the game specifications yet! Feel free to always refer back to https://www.lux-ai.org/specs-s2.\n\n## Basic Setup\nFirst thing to verify is that you have python 3.7 or above and have the [luxai_s2](https://pypi.org/project/luxai_s2/) package installed. Run the command below to do so. If you are using Kaggle Notebooks, **make sure to also click run-> restart and clear cell outputs** on the top right next to view and add-ons. (This fixes a bug where Kaggle Notebooks loads an incompatible package)","metadata":{}},{"cell_type":"code","source":"# verify version\n!python --version\n!pip install --upgrade luxai_s2\n!cp -r ../input/lux-ai-season-2/* .","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:18:35.930297Z","iopub.execute_input":"2023-09-20T21:18:35.930844Z","iopub.status.idle":"2023-09-20T21:19:03.319413Z","shell.execute_reply.started":"2023-09-20T21:18:35.930698Z","shell.execute_reply":"2023-09-20T21:19:03.317545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from luxai_s2.env import LuxAI_S2\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:19:03.322229Z","iopub.execute_input":"2023-09-20T21:19:03.322828Z","iopub.status.idle":"2023-09-20T21:19:04.047881Z","shell.execute_reply.started":"2023-09-20T21:19:03.322791Z","shell.execute_reply":"2023-09-20T21:19:04.046666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now create an environment and start interacting with it, as well as look at what the observation is like","metadata":{}},{"cell_type":"code","source":"env = LuxAI_S2() # create the environment object\nobs, _ = env.reset(seed=41) # resets an environment with a seed","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:19:04.049481Z","iopub.execute_input":"2023-09-20T21:19:04.049849Z","iopub.status.idle":"2023-09-20T21:19:04.071341Z","shell.execute_reply.started":"2023-09-20T21:19:04.049818Z","shell.execute_reply":"2023-09-20T21:19:04.069842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the observation is always composed of observations for both players.\nobs.keys(), obs[\"player_0\"].keys()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:19:04.074370Z","iopub.execute_input":"2023-09-20T21:19:04.074762Z","iopub.status.idle":"2023-09-20T21:19:04.083522Z","shell.execute_reply.started":"2023-09-20T21:19:04.074727Z","shell.execute_reply":"2023-09-20T21:19:04.082390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To visualize the environment, on jupyter notebooks you have two options\n\nWith the `rgb_array` mode you can visualize every step as an environment episode progresses. \n\nWith the CLI tool, you can run an episode and save a replay.json to upload to https://s2vis.lux-ai.org/ or a replay.html file to directly open and watch","metadata":{}},{"cell_type":"code","source":"# visualize the environment so far with rgb_array to get a quick look at the map\n# dark orange - high rubble, light orange - low rubble\n# blue = ice, yellow = ore\nimg = env.render()\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:19:04.085208Z","iopub.execute_input":"2023-09-20T21:19:04.085554Z","iopub.status.idle":"2023-09-20T21:19:04.531036Z","shell.execute_reply.started":"2023-09-20T21:19:04.085525Z","shell.execute_reply":"2023-09-20T21:19:04.529886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building an Agent\nNow we know what the environment looks like, let's try building a working agent. The goal of this environment to ensure at least one factory stays alive by the end of the episode and grow as much lichen as possible.\n\nIn our kit we provide a skeleton for building an agent. Avoid removing any function from the kit unless you know what you are doing as it may cause your agent to fail on the competition servers. This agent defintion should be stored in the `agent.py` file.\n\nThe agent will have `self.player, self.opp_player, self.env_cfg` populated with the correct values at each step of an environment during competition or when you use the CLI tool to run matches. \n\n`self.env_cfg` stores the curent environment's configurations, and `self.player, self.opp_player` stores the name of your player/team and the opposition respectively (will always be \"player_0\" or \"player_1\").","metadata":{}},{"cell_type":"code","source":"from lux.kit import obs_to_game_state, GameState, EnvConfig\nfrom luxai_s2.utils import animate\nfrom lux.utils import direction_to, my_turn_to_place_factory\nclass Agent():\n    def __init__(self, player: str, env_cfg: EnvConfig) -> None:\n        self.player = player\n        self.opp_player = \"player_1\" if self.player == \"player_0\" else \"player_0\"\n        np.random.seed(0)\n        self.env_cfg: EnvConfig = env_cfg\n\n    def early_setup(self, step: int, obs, remainingOverageTime: int = 60):\n        actions = dict()\n        # optionally convert observations to python objects with utility functions\n        game_state = obs_to_game_state(step, self.env_cfg, obs) \n        return actions\n\n    def act(self, step: int, obs, remainingOverageTime: int = 60):\n        actions = dict()\n        game_state = obs_to_game_state(step, self.env_cfg, obs)\n        return actions","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:19:04.532541Z","iopub.execute_input":"2023-09-20T21:19:04.533767Z","iopub.status.idle":"2023-09-20T21:19:04.565161Z","shell.execute_reply.started":"2023-09-20T21:19:04.533715Z","shell.execute_reply":"2023-09-20T21:19:04.563574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that in the environment, there are two distinct phases of the game where you will have to program two different sets of logic to play it.\n\nWe will also define a simple function to initialize our agent and interact with the environment and generate a simple video replay.\nNo need to worry about how this works specifically, you can copy paste this as you see fit. Note that this is a simplified representation. In order to visually see all numbers and details about units, factories etc. use our web visualizer.","metadata":{}},{"cell_type":"code","source":"def animate(imgs, _return=True):\n    # using cv2 to generate videos as moviepy doesn't work on kaggle notebooks\n    import cv2\n    import os\n    import string\n    import random\n    video_name = ''.join(random.choice(string.ascii_letters) for i in range(18))+'.webm'\n    height, width, layers = imgs[0].shape\n    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n    video = cv2.VideoWriter(video_name, fourcc, 10, (width,height))\n\n    for img in imgs:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        video.write(img)\n    video.release()\n    if _return:\n        from IPython.display import Video\n        return Video(video_name)\ndef interact(env, agents, steps):\n    # reset our env\n    obs, _ = env.reset()\n    np.random.seed(0)\n    imgs = []\n    step = 0\n    # Note that as the environment has two phases, we also keep track a value called \n    # `real_env_steps` in the environment state. The first phase ends once `real_env_steps` is 0 and used below\n\n    # iterate until phase 1 ends\n    while env.state.real_env_steps < 0:\n        if step >= steps: break\n        actions = {}\n        for player in env.agents:\n            o = obs[player]\n            a = agents[player].early_setup(step, o)\n            actions[player] = a\n        step += 1\n        obs, rewards, terminations, truncations, infos = env.step(actions)\n        imgs += [env.render()]\n#         print(termination)\n        dones = {k: terminations[k] or truncations[k] for k in terminations}\n    done = False\n    while not done:\n        if step >= steps: break\n        actions = {}\n        for player in env.agents:\n            o = obs[player]\n            a = agents[player].act(step, o)\n            actions[player] = a\n        step += 1\n        obs, rewards, terminations, truncations, infos = env.step(actions)\n        imgs += [env.render()]\n        dones = {k: terminations[k] or truncations[k] for k in terminations}\n        done = dones[\"player_0\"] and dones[\"player_1\"]\n    return animate(imgs)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:22:03.623295Z","iopub.execute_input":"2023-09-20T21:22:03.623879Z","iopub.status.idle":"2023-09-20T21:22:03.645929Z","shell.execute_reply.started":"2023-09-20T21:22:03.623830Z","shell.execute_reply":"2023-09-20T21:22:03.644602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Early Phase\n\nDuring the first turn of the game, each player is given the map, starting resources (N factories and N\\*150 water and ore), and are asked to bid for who goes first/second. Each 1 bid removes 1 water and 1 ore from that player's starting resources. Each player responds in turn 1 with their bid, which can be positive to prefer going first or negative to prefer going second. \n\nAfter bidding you then place each of your factories. Each team gets `N` factories to place, with. For conveniency the observation contains all possible spawn locations for your team which account for the opponent's factories and resource tiles which you can't spawn on.\n\nWe will write a simple `early_setup` function to return the appropriate action to handle this phase.\n\n","metadata":{}},{"cell_type":"code","source":"def early_setup(self, step: int, obs, remainingOverageTime: int = 60):\n    if step == 0:\n        # bid 0 to not waste resources bidding and declare as the default faction\n        # you can bid -n to prefer going second or n to prefer going first in placement\n        return dict(faction=\"AlphaStrike\", bid=0)\n    else:\n        game_state = obs_to_game_state(step, self.env_cfg, obs)\n        # factory placement period\n        \n        # how much water and metal you have in your starting pool to give to new factories\n        water_left = game_state.teams[self.player].water\n        metal_left = game_state.teams[self.player].metal\n        \n        # how many factories you have left to place\n        factories_to_place = game_state.teams[self.player].factories_to_place\n        # whether it is your turn to place a factory\n        my_turn_to_place = my_turn_to_place_factory(game_state.teams[self.player].place_first, step)\n        if factories_to_place > 0 and my_turn_to_place:\n            # we will spawn our factory in a random location with 150 metal and water if it is our turn to place\n            potential_spawns = np.array(list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1))))\n            spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]\n            return dict(spawn=spawn_loc, metal=150, water=150)\n        return dict()\nAgent.early_setup = early_setup","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:22:04.191168Z","iopub.execute_input":"2023-09-20T21:22:04.191626Z","iopub.status.idle":"2023-09-20T21:22:04.202944Z","shell.execute_reply.started":"2023-09-20T21:22:04.191586Z","shell.execute_reply":"2023-09-20T21:22:04.201505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each team gets `N` factories to place, with. For conveniency the observation contains all possible spawn locations for your team","metadata":{}},{"cell_type":"code","source":"# recreate our agents and run\nagents = {player: Agent(player, env.state.env_cfg) for player in env.agents}\ninteract(env, agents, 15)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:22:05.021832Z","iopub.execute_input":"2023-09-20T21:22:05.022319Z","iopub.status.idle":"2023-09-20T21:22:07.205777Z","shell.execute_reply.started":"2023-09-20T21:22:05.022281Z","shell.execute_reply":"2023-09-20T21:22:07.204926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Congratz, we have lift off! We got both teams to spawn some factories. With the early phase over, we can now start programming the logic to power the next phase.\n\n### Regular Phase\n\nThe goal of the game is to grow more lichen than your opponent by the end of the 1000 step episode (not including early phase steps). To grow lichen factories must consume water. Moreover, factories passively consume 1 water a turn, and you must ensure at least one factory survives until the end. Whichever team loses all their factories first will automatically lose. \n\nTo obtain water, robots/units must mine ice from ice tiles (blue on the map) and deliver them back to a factory which then automatically refines ice into water. \n\nMoreover, to help us write better code, we will use the provided function `obs_to_game_state` to convert observations from raw dictionaries to interactable python objects. Finally, we will also use a provided `animate` function to easily generate a simple video of the episode and embed it into here without having to upload a replay file to the web visualizer.","metadata":{}},{"cell_type":"markdown","source":"#### Building Robots\n\nOnly factories can build robots, so for each factory on our team, if there is enough metal and power, we will issue a command to build a new heavy robot.\n\nMore advanced strategies will be able to efficiently leverage light units as well to collect resources but for simplicity, this tutorial uses heavy robots since they don't need to move as often to collect many resources but can collect many at a time.","metadata":{}},{"cell_type":"code","source":"from lux.kit import obs_to_game_state, GameState\ndef act(self, step: int, obs, remainingOverageTime: int = 60):\n    actions = dict()\n    game_state: GameState = obs_to_game_state(step, self.env_cfg, obs)\n    factories = game_state.factories[self.player]\n    for unit_id, factory in factories.items():\n        if factory.power >= self.env_cfg.ROBOTS[\"HEAVY\"].POWER_COST and \\\n        factory.cargo.metal >= self.env_cfg.ROBOTS[\"HEAVY\"].METAL_COST:\n            actions[unit_id] = factory.build_heavy()\n    return actions\nAgent.act = act","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:22:35.689879Z","iopub.execute_input":"2023-09-20T21:22:35.690778Z","iopub.status.idle":"2023-09-20T21:22:35.700182Z","shell.execute_reply.started":"2023-09-20T21:22:35.690736Z","shell.execute_reply":"2023-09-20T21:22:35.698626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recreate our agents and run\nagents = {player: Agent(player, env.state.env_cfg) for player in env.agents}\ninteract(env, agents, 25)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:22:39.172667Z","iopub.execute_input":"2023-09-20T21:22:39.173223Z","iopub.status.idle":"2023-09-20T21:22:42.979728Z","shell.execute_reply.started":"2023-09-20T21:22:39.173181Z","shell.execute_reply":"2023-09-20T21:22:42.978343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Robots built, but they're idle! Let's get them to work.\n\n#### Moving Robots and Mining\n\nWe want the robots to find the closest ice tile and mine it.\nWe'll worry about delivering the ice back home later. Let's update our `act` function to add this functionality. We will iterate over all our units and find the closest ice tile. Then we will use a given utility function that gives us the direction that takes us towards the desired ice tile. Lastly, we check if we have enough power to move and can move in that direction and submit that action.\n\nImportantly, in this season **units are given action queues (a list of actions). Each time an action queue is given, the unit's old action queue is replaced completely in addition to a small additional action queue submission power cost.**\n\nMoreover, each action has two other attributes, its execution count `n` and `repeat`. Each time the action is succesfully executed, we decrement `n` by 1. If `n` hits 0, we remove the action from the action queue. If `repeat == 0`, then we don't recycle the action. If `repeat > 0`, then we **recycle the same action to the back of the action queue** but this time with `n = repeat`.\n\nFor this tutorial, we will be giving units one action at a time with no action recycling or multiple executions. For more advanced competitors, to reduce power costs you may want to submit longer action queues and only update the action queue every once in a while.","metadata":{}},{"cell_type":"code","source":"from lux.utils import direction_to\nimport sys\ndef act(self, step: int, obs, remainingOverageTime: int = 60):\n    actions = dict()\n    game_state = obs_to_game_state(step, self.env_cfg, obs)\n    factories = game_state.factories[self.player]\n    for unit_id, factory in factories.items():\n        if factory.power >= self.env_cfg.ROBOTS[\"HEAVY\"].POWER_COST and \\\n        factory.cargo.metal >= self.env_cfg.ROBOTS[\"HEAVY\"].METAL_COST:\n            actions[unit_id] = factory.build_heavy()\n            \n    # iterate over our units and have them mine the closest ice tile\n    units = game_state.units[self.player]\n    ice_map = game_state.board.ice # flip the board as it stores by rows then columns\n    ice_tile_locations = np.argwhere(ice_map == 1) # numpy magic to get the position of every ice tile\n    for unit_id, unit in units.items():\n        # compute the distance to each ice tile from this unit and pick the closest\n        ice_tile_distances = np.mean((ice_tile_locations - unit.pos) ** 2, 1)\n        closest_ice_tile = ice_tile_locations[np.argmin(ice_tile_distances)]\n        \n        # if we have reached the ice tile, start mining if possible\n        if np.all(closest_ice_tile == unit.pos):\n            if unit.power >= unit.dig_cost(game_state) + unit.action_queue_cost(game_state):\n                actions[unit_id] = [unit.dig(repeat=0, n=1)]\n        else:\n            direction = direction_to(unit.pos, closest_ice_tile)\n            move_cost = unit.move_cost(game_state, direction)\n            # check move_cost is not None, meaning that direction is not off the map or blocked\n            # check if unit has enough power to move in addition to updating the action queue.\n            if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):\n                actions[unit_id] = [unit.move(direction, repeat=0, n=1)]\n        # since we are using the simple embedded visualizer, we will have to print out details about units\n        # importantly, note that we print with file=sys.stderr. Printing with anything will cause your agent to fail\n        if unit.cargo.ice > 50:\n            print(game_state.real_env_steps, unit, f\"has {unit.cargo.ice} ice\", file=sys.stderr)\n    return actions\nAgent.act = act","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:22:50.481073Z","iopub.execute_input":"2023-09-20T21:22:50.481567Z","iopub.status.idle":"2023-09-20T21:22:50.498525Z","shell.execute_reply.started":"2023-09-20T21:22:50.481528Z","shell.execute_reply":"2023-09-20T21:22:50.496741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recreate our agents and run\nagents = {player: Agent(player, env.state.env_cfg) for player in env.agents}\ninteract(env, agents, steps=40)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:22:57.931640Z","iopub.execute_input":"2023-09-20T21:22:57.932143Z","iopub.status.idle":"2023-09-20T21:23:04.338591Z","shell.execute_reply.started":"2023-09-20T21:22:57.932089Z","shell.execute_reply":"2023-09-20T21:23:04.337158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And they're off! The heavy robots have started to move towards the ice tiles and some have begun mining.\n\n#### Delivering Resources and Keep Factories Alive\nWe now have ice being mined, but we now need to deliver that back to the factories so they can refine that ice into water and sustain themselves.","metadata":{}},{"cell_type":"code","source":"def act(self, step: int, obs, remainingOverageTime: int = 60):\n    actions = dict()\n    game_state = obs_to_game_state(step, self.env_cfg, obs)\n    factories = game_state.factories[self.player]\n    factory_tiles, factory_units = [], []\n    for unit_id, factory in factories.items():\n        if factory.power >= self.env_cfg.ROBOTS[\"HEAVY\"].POWER_COST and \\\n        factory.cargo.metal >= self.env_cfg.ROBOTS[\"HEAVY\"].METAL_COST:\n            actions[unit_id] = factory.build_heavy()\n        factory_tiles += [factory.pos]\n        factory_units += [factory]\n    factory_tiles = np.array(factory_tiles)\n\n    units = game_state.units[self.player]\n    ice_map = game_state.board.ice\n    ice_tile_locations = np.argwhere(ice_map == 1)\n    for unit_id, unit in units.items():\n        \n        # track the closest factory\n        closest_factory = None\n        adjacent_to_factory = False\n        if len(factory_tiles) > 0:\n            factory_distances = np.mean((factory_tiles - unit.pos) ** 2, 1)\n            closest_factory_tile = factory_tiles[np.argmin(factory_distances)]\n            closest_factory = factory_units[np.argmin(factory_distances)]\n            adjacent_to_factory = np.mean((closest_factory_tile - unit.pos) ** 2) == 0\n        \n            # previous ice mining code\n            if unit.cargo.ice < 40:\n                ice_tile_distances = np.mean((ice_tile_locations - unit.pos) ** 2, 1)\n                closest_ice_tile = ice_tile_locations[np.argmin(ice_tile_distances)]\n                if np.all(closest_ice_tile == unit.pos):\n                    if unit.power >= unit.dig_cost(game_state) + unit.action_queue_cost(game_state):\n                        actions[unit_id] = [unit.dig(repeat=0, n=1)]\n                else:\n                    direction = direction_to(unit.pos, closest_ice_tile)\n                    move_cost = unit.move_cost(game_state, direction)\n                    if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):\n                        actions[unit_id] = [unit.move(direction, repeat=0, n=1)]\n            # else if we have enough ice, we go back to the factory and dump it.\n            elif unit.cargo.ice >= 40:\n                direction = direction_to(unit.pos, closest_factory_tile)\n                if adjacent_to_factory:\n                    if unit.power >= unit.action_queue_cost(game_state):\n                        actions[unit_id] = [unit.transfer(direction, 0, unit.cargo.ice, repeat=0, n=1)]\n                else:\n                    move_cost = unit.move_cost(game_state, direction)\n                    if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):\n                        actions[unit_id] = [unit.move(direction, repeat=0, n=1)]\n    return actions\nAgent.act = act","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:23:23.891147Z","iopub.execute_input":"2023-09-20T21:23:23.891747Z","iopub.status.idle":"2023-09-20T21:23:23.920593Z","shell.execute_reply.started":"2023-09-20T21:23:23.891699Z","shell.execute_reply":"2023-09-20T21:23:23.919325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recreate our agents and run\nagents = {player: Agent(player, env.state.env_cfg) for player in env.agents}\ninteract(env, agents, 300)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:23:29.304833Z","iopub.execute_input":"2023-09-20T21:23:29.305316Z","iopub.status.idle":"2023-09-20T21:23:56.858715Z","shell.execute_reply.started":"2023-09-20T21:23:29.305280Z","shell.execute_reply":"2023-09-20T21:23:56.856939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some factories are surviving for for more than 150 steps thanks to the delivery of additional ice, but more work will need to be done to keep them alive longer.\n\nPuting all those pieces together the full starter agent looks like this (and we will save it to agent.py)","metadata":{}},{"cell_type":"code","source":"%%writefile agent.py\nfrom lux.kit import obs_to_game_state, GameState, EnvConfig\nfrom lux.utils import direction_to, my_turn_to_place_factory\nimport numpy as np\nimport sys\nclass Agent():\n    def __init__(self, player: str, env_cfg: EnvConfig) -> None:\n        self.player = player\n        self.opp_player = \"player_1\" if self.player == \"player_0\" else \"player_0\"\n        np.random.seed(0)\n        self.env_cfg: EnvConfig = env_cfg\n\n    def early_setup(self, step: int, obs, remainingOverageTime: int = 60):\n        if step == 0:\n            # bid 0 to not waste resources bidding and declare as the default faction\n            return dict(faction=\"AlphaStrike\", bid=0)\n        else:\n            game_state = obs_to_game_state(step, self.env_cfg, obs)\n            # factory placement period\n\n            # how much water and metal you have in your starting pool to give to new factories\n            water_left = game_state.teams[self.player].water\n            metal_left = game_state.teams[self.player].metal\n\n            # how many factories you have left to place\n            factories_to_place = game_state.teams[self.player].factories_to_place\n            # whether it is your turn to place a factory\n            my_turn_to_place = my_turn_to_place_factory(game_state.teams[self.player].place_first, step)\n            if factories_to_place > 0 and my_turn_to_place:\n                # we will spawn our factory in a random location with 150 metal and water if it is our turn to place\n                potential_spawns = np.array(list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1))))\n                spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]\n                return dict(spawn=spawn_loc, metal=150, water=150)\n            return dict()\n\n    def act(self, step: int, obs, remainingOverageTime: int = 60):\n        actions = dict()\n        game_state = obs_to_game_state(step, self.env_cfg, obs)\n        factories = game_state.factories[self.player]\n        game_state.teams[self.player].place_first\n        factory_tiles, factory_units = [], []\n        for unit_id, factory in factories.items():\n            if factory.power >= self.env_cfg.ROBOTS[\"HEAVY\"].POWER_COST and \\\n            factory.cargo.metal >= self.env_cfg.ROBOTS[\"HEAVY\"].METAL_COST:\n                actions[unit_id] = factory.build_heavy()\n            if self.env_cfg.max_episode_length - game_state.real_env_steps < 50:\n                if factory.water_cost(game_state) <= factory.cargo.water:\n                    actions[unit_id] = factory.water()\n            factory_tiles += [factory.pos]\n            factory_units += [factory]\n        factory_tiles = np.array(factory_tiles)\n\n        units = game_state.units[self.player]\n        ice_map = game_state.board.ice\n        ice_tile_locations = np.argwhere(ice_map == 1)\n        for unit_id, unit in units.items():\n\n            # track the closest factory\n            closest_factory = None\n            adjacent_to_factory = False\n            if len(factory_tiles) > 0:\n                factory_distances = np.mean((factory_tiles - unit.pos) ** 2, 1)\n                closest_factory_tile = factory_tiles[np.argmin(factory_distances)]\n                closest_factory = factory_units[np.argmin(factory_distances)]\n                adjacent_to_factory = np.mean((closest_factory_tile - unit.pos) ** 2) == 0\n\n                # previous ice mining code\n                if unit.cargo.ice < 40:\n                    ice_tile_distances = np.mean((ice_tile_locations - unit.pos) ** 2, 1)\n                    closest_ice_tile = ice_tile_locations[np.argmin(ice_tile_distances)]\n                    if np.all(closest_ice_tile == unit.pos):\n                        if unit.power >= unit.dig_cost(game_state) + unit.action_queue_cost(game_state):\n                            actions[unit_id] = [unit.dig(repeat=0, n=1)]\n                    else:\n                        direction = direction_to(unit.pos, closest_ice_tile)\n                        move_cost = unit.move_cost(game_state, direction)\n                        if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):\n                            actions[unit_id] = [unit.move(direction, repeat=0, n=1)]\n                # else if we have enough ice, we go back to the factory and dump it.\n                elif unit.cargo.ice >= 40:\n                    direction = direction_to(unit.pos, closest_factory_tile)\n                    if adjacent_to_factory:\n                        if unit.power >= unit.action_queue_cost(game_state):\n                            actions[unit_id] = [unit.transfer(direction, 0, unit.cargo.ice, repeat=0, n=1)]\n                    else:\n                        move_cost = unit.move_cost(game_state, direction)\n                        if move_cost is not None and unit.power >= move_cost + unit.action_queue_cost(game_state):\n                            actions[unit_id] = [unit.move(direction, repeat=0, n=1)]\n        return actions","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:23:56.861407Z","iopub.execute_input":"2023-09-20T21:23:56.861919Z","iopub.status.idle":"2023-09-20T21:23:56.875749Z","shell.execute_reply.started":"2023-09-20T21:23:56.861875Z","shell.execute_reply":"2023-09-20T21:23:56.874398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a submission\nNow we need to create a .tar.gz file with main.py (and agent.py) at the top level. We can then upload this!","metadata":{}},{"cell_type":"code","source":"!tar -czf submission.tar.gz *","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:23:56.878376Z","iopub.execute_input":"2023-09-20T21:23:56.878842Z","iopub.status.idle":"2023-09-20T21:23:58.054581Z","shell.execute_reply.started":"2023-09-20T21:23:56.878751Z","shell.execute_reply":"2023-09-20T21:23:58.052757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit\nNow open the /kaggle/working folder and find submission.tar.gz, download that file, navigate to the \"MySubmissions\" tab in https://www.kaggle.com/competitions/lux-ai-season-2-neurips-stage-2/submissions and upload your submission! It should play a validation match against itself and once it succeeds it will be automatically matched against other players' submissions. Newer submissions will be prioritized for games over older ones. Your team is limited in the number of succesful submissions per day so we highly recommend testing your bot locally before submitting.","metadata":{}},{"cell_type":"markdown","source":"## CLI Tool\n\nTo test your agent without using the python API you can also run","metadata":{}},{"cell_type":"code","source":"!luxai-s2 main.py main.py -v 2 -s 101 -o replay.html","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:24:41.993571Z","iopub.execute_input":"2023-09-20T21:24:41.994134Z","iopub.status.idle":"2023-09-20T21:24:48.145177Z","shell.execute_reply.started":"2023-09-20T21:24:41.994080Z","shell.execute_reply":"2023-09-20T21:24:48.144018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"which uses a seed of 101 and generates a replay.html file that you can click and watch. Optionally if you specify `-o replay.json` you can upload replay.json to http://s2vis.lux-ai.org/. We **highly recommend** watching on a separate window instead of watching here on a notebook as the notebook screen width is quite small.\n\nThe CLI tool enables you to easily run episodes between any two agents (python or not) and provides a flexible tournament running tool to evaluate many agents together. Documentation on this tool can be found here: https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/luxai_runner/README.md","metadata":{}},{"cell_type":"code","source":"import IPython # load the HTML replay\nIPython.display.HTML(filename='replay.html')","metadata":{"execution":{"iopub.status.busy":"2023-09-20T21:27:05.392146Z","iopub.execute_input":"2023-09-20T21:27:05.392612Z","iopub.status.idle":"2023-09-20T21:27:05.420684Z","shell.execute_reply.started":"2023-09-20T21:27:05.392579Z","shell.execute_reply":"2023-09-20T21:27:05.419274Z"},"trusted":true},"execution_count":null,"outputs":[]}]}